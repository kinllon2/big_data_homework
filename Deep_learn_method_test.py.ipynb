{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3341557",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random_forest.random_forest_model import RandomForest\n",
    "\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import nltk\n",
    "import string\n",
    "from nltk import ngrams\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfeb8fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local path of the data source\n",
    "testing_data = 'data/testing.csv'\n",
    "training_data = 'data/training.csv'\n",
    "validation_data = 'data/validation.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac03e349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_clean(review): \n",
    "    # changing to lower case\n",
    "    lower = review.str.lower()\n",
    "    # Replacing the repeating pattern of &#039;\n",
    "    pattern_remove = lower.str.replace(\"&#039;\", \"\")\n",
    "    # Removing all the special Characters\n",
    "    special_remove = pattern_remove.str.replace(r'[^\\w\\d\\s]',' ')\n",
    "    # Removing all the non ASCII characters\n",
    "    ascii_remove = special_remove.str.replace(r'[^\\x00-\\x7F]+',' ')\n",
    "    # Removing the leading and trailing Whitespaces\n",
    "    whitespace_remove = ascii_remove.str.replace(r'^\\s+|\\s+?$','')\n",
    "    # Replacing multiple Spaces with Single Space\n",
    "    multiw_remove = whitespace_remove.str.replace(r'\\s+',' ')\n",
    "    # Replacing Two or more dots with one\n",
    "    dataframe = multiw_remove.str.replace(r'\\.{2,}', ' ')\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c355091b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(review):\n",
    "    # Sentiment polarity of the reviews\n",
    "    pol = []\n",
    "    for i in review:\n",
    "        analysis = TextBlob(i)\n",
    "        pol.append(analysis.sentiment.polarity)\n",
    "    return pol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9619e11b",
   "metadata": {},
   "source": [
    "### train data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27a6e369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    4155\n",
       "1.0    4004\n",
       "Name: Review_Sentiment, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# three data given by the COMP5434 project\n",
    "train = pd.read_csv(training_data)\n",
    "validation = pd.read_csv(validation_data)\n",
    "test = pd.read_csv(training_data)\n",
    "\n",
    "data = pd.concat([train, validation])\n",
    "data['review_clean'] = review_clean(data['reviewComment'])\n",
    "# Removing the stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "data['review_clean'] = data['review_clean'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))\n",
    "# Removing the word stems using the Snowball Stemmer\n",
    "Snow_ball = SnowballStemmer(\"english\")\n",
    "data['review_clean'] = data['review_clean'].apply(lambda x: \" \".join(Snow_ball.stem(word) for word in x.split()))\n",
    "\n",
    "data['sentiment'] = sentiment(data['reviewComment'])\n",
    "data['sentiment_clean'] = sentiment(data['review_clean'])\n",
    "# Cleaning the reviews without removing the stop words and using snowball stemmer\n",
    "data['review_clean_ss'] = review_clean(data['reviewComment'])\n",
    "data['sentiment_clean_ss'] = sentiment(data['review_clean_ss'])\n",
    "data = data.dropna(how=\"any\", axis=0)\n",
    "#Word count in each review\n",
    "data['count_word']=data[\"review_clean_ss\"].apply(lambda x: len(str(x).split()))\n",
    "#Unique word count \n",
    "data['count_unique_word']=data[\"review_clean_ss\"].apply(lambda x: len(set(str(x).split())))\n",
    "#Letter count\n",
    "data['count_letters']=data[\"review_clean_ss\"].apply(lambda x: len(str(x)))\n",
    "#punctuation count\n",
    "data[\"count_punctuations\"] = data[\"reviewComment\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "#upper case words count\n",
    "data[\"count_words_upper\"] = data[\"reviewComment\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "#title case words count\n",
    "data[\"count_words_title\"] = data[\"reviewComment\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "#Number of stopwords\n",
    "data[\"count_stopwords\"] = data[\"reviewComment\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stop_words]))\n",
    "#Average length of the words\n",
    "data[\"mean_word_len\"] = data[\"review_clean_ss\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "# Label Encoding Drugname and Conditions\n",
    "label_encoder_feat = {}\n",
    "for feature in ['drugName', 'condition']:\n",
    "    label_encoder_feat[feature] = LabelEncoder()\n",
    "    data[feature] = label_encoder_feat[feature].fit_transform(data[feature])\n",
    "\n",
    "# converting the date into datetime format\n",
    "data['date'] = pd.to_datetime(data['date'], errors = 'coerce')\n",
    "\n",
    "# now extracting year from date\n",
    "data['Year'] = data['date'].dt.year\n",
    "\n",
    "# extracting the month from the date\n",
    "data['month'] = data['date'].dt.month\n",
    "\n",
    "# extracting the days from the date\n",
    "data['day'] = data['date'].dt.day\n",
    "\n",
    "data.loc[(data['rating'] >= 5), 'Review_Sentiment'] = 1\n",
    "data.loc[(data['rating'] < 5), 'Review_Sentiment'] = 0\n",
    "\n",
    "data['Review_Sentiment'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee41661a",
   "metadata": {},
   "source": [
    "### test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8688483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    3553\n",
       "1.0    3412\n",
       "Name: Review_Sentiment, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# three data given by the COMP5434 project\n",
    "\n",
    "test['review_clean'] = review_clean(test['reviewComment'])\n",
    "# Removing the stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "test['review_clean'] = test['review_clean'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))\n",
    "# Removing the word stems using the Snowball Stemmer\n",
    "Snow_ball = SnowballStemmer(\"english\")\n",
    "test['review_clean'] = test['review_clean'].apply(lambda x: \" \".join(Snow_ball.stem(word) for word in x.split()))\n",
    "\n",
    "test['sentiment'] = sentiment(test['reviewComment'])\n",
    "test['sentiment_clean'] = sentiment(test['review_clean'])\n",
    "# Cleaning the reviews without removing the stop words and using snowball stemmer\n",
    "test['review_clean_ss'] = review_clean(test['reviewComment'])\n",
    "test['sentiment_clean_ss'] = sentiment(test['review_clean_ss'])\n",
    "test = test.dropna(how=\"any\", axis=0)\n",
    "#Word count in each review\n",
    "test['count_word']=test[\"review_clean_ss\"].apply(lambda x: len(str(x).split()))\n",
    "#Unique word count \n",
    "test['count_unique_word']=test[\"review_clean_ss\"].apply(lambda x: len(set(str(x).split())))\n",
    "#Letter count\n",
    "test['count_letters']=test[\"review_clean_ss\"].apply(lambda x: len(str(x)))\n",
    "#punctuation count\n",
    "test[\"count_punctuations\"] = test[\"reviewComment\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "#upper case words count\n",
    "test[\"count_words_upper\"] = test[\"reviewComment\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "#title case words count\n",
    "test[\"count_words_title\"] = test[\"reviewComment\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "#Number of stopwords\n",
    "test[\"count_stopwords\"] = test[\"reviewComment\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stop_words]))\n",
    "#Average length of the words\n",
    "test[\"mean_word_len\"] = test[\"review_clean_ss\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "# Label Encoding Drugname and Conditions\n",
    "label_encoder_feat = {}\n",
    "for feature in ['drugName', 'condition']:\n",
    "    label_encoder_feat[feature] = LabelEncoder()\n",
    "    test[feature] = label_encoder_feat[feature].fit_transform(test[feature])\n",
    "\n",
    "# converting the date into datetime format\n",
    "test['date'] = pd.to_datetime(test['date'], errors = 'coerce')\n",
    "\n",
    "# now extracting year from date\n",
    "test['Year'] = test['date'].dt.year\n",
    "\n",
    "# extracting the month from the date\n",
    "test['month'] = test['date'].dt.month\n",
    "\n",
    "# extracting the days from the date\n",
    "test['day'] = test['date'].dt.day\n",
    "\n",
    "test.loc[(test['rating'] >= 5), 'Review_Sentiment'] = 1\n",
    "test.loc[(test['rating'] < 5), 'Review_Sentiment'] = 0\n",
    "\n",
    "test['Review_Sentiment'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "366c7df4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining data shape : \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mX_train\u001b[49m\u001b[38;5;241m.\u001b[39mshape, y_train\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTesting data shape : \u001b[39m\u001b[38;5;124m'\u001b[39m, X_test\u001b[38;5;241m.\u001b[39mshape, y_test\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "print('Training data shape : ', X_train.shape, y_train.shape)\n",
    "print('Testing data shape : ', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8acea99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"I039ve tried a few antidepressants over the years citalopram, fluoxetine, amitriptyline, but none of those helped with my depression, insomnia amp anxiety My doctor suggested and changed me onto 45mg mirtazapine and this medicine has saved my life Thankfully I have had no side effects especially the most common  weight gain, I039ve actually lost alot of weight I still have suicidal thoughts but mirtazapine has saved me\"'\n",
      " '\"My son has Crohn039s disease and has done very well on the Asacol  He has no complaints and shows no side effects  He has taken as many as nine tablets per day at one time  I039ve been very happy with the results, reducing his bouts of diarrhea drastically\"']\n"
     ]
    }
   ],
   "source": [
    "b = \"'@#$%^()&*;!.-\"\n",
    "X_train = np.array(train['reviewComment'])\n",
    "X_test = np.array(validation['reviewComment'])\n",
    "\n",
    "def clean(X):\n",
    "    for index, review in enumerate(X):\n",
    "        for char in b:\n",
    "            X[index] = X[index].replace(char, \"\")\n",
    "    return(X)\n",
    "\n",
    "X_train = clean(X_train)\n",
    "X_test = clean(X_test)\n",
    "print(X_train[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32d51663",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 14:47:18.772116: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-01 14:47:18.772157: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8198,)\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.cluster import KMeansClusterer\n",
    "import nltk\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True, stop_words=stopwords.words('english'),lowercase=True, max_features=5000)\n",
    "#vectorizer = TfidfVectorizer(binary=True, stop_words=stopwords.words('english'), lowercase=True, max_features=5000)\n",
    "test_train = np.concatenate([X_train, X_test])\n",
    "print(test_train.shape)\n",
    "X_onehot = vectorizer.fit_transform(test_train)\n",
    "stop_words = vectorizer.get_stop_words()\n",
    "print(type(X_onehot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28bb369",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_onehot.shape)\n",
    "print(X_onehot.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea0f9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_list = vectorizer.get_feature_names()\n",
    "names = [[i] for i in names_list]\n",
    "names = Word2Vec(names, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34abd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_transform(X):\n",
    "    y_reshaped = np.reshape(X['rating'].values, (-1, 1))\n",
    "    for index, val in enumerate(y_reshaped):\n",
    "        if val >= 8:\n",
    "            y_reshaped[index] = 1\n",
    "        elif val >= 5:\n",
    "            y_reshaped[index] = 2\n",
    "        else:\n",
    "            y_reshaped[index] = 0\n",
    "    y_result = to_categorical(y_reshaped)\n",
    "    return y_result\n",
    "    \n",
    "    print(X_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a198b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_test = pd.concat([train, test], ignore_index=True)\n",
    "y_train = score_transform(y_train_test)\n",
    "print(y_train)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002baac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "\n",
    "np.random.seed(1)\n",
    "model = Sequential()\n",
    "model.add(Dense(units=256, activation='relu', input_dim=len(vectorizer.get_feature_names())))\n",
    "model.add(Dense(units=3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aab67e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "X_onehot = tf.sparse.reorder(X_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0bf5de93",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data[['condition', 'usefulCount', 'sentiment', 'day', 'month', 'Year',\n",
    "                'sentiment_clean_ss', 'count_word', 'count_unique_word', 'count_letters',\n",
    "                'count_punctuations', 'count_words_upper', 'count_words_title',\n",
    "                'count_stopwords', 'mean_word_len', 'rating', 'review_clean']]\n",
    "df_train, df_test = train_test_split(features, test_size=0.33, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4778a2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = 'word', \n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None, \n",
    "                             stop_words = None, \n",
    "                             min_df = 2, # 토큰이 나타날 최소 문서 개수\n",
    "                             ngram_range=(4, 4),\n",
    "                             max_features = 20000\n",
    "                            )\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2f099c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/28160335/plot-a-document-tfidf-2d-graph\n",
    "pipeline = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52803179",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time train_data_features = pipeline.fit_transform(df_train['review_clean'])\n",
    "%time test_data_features = pipeline.fit_transform(df_test['review_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b75fcb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_data_features \u001b[38;5;241m=\u001b[39m \u001b[43mdf_train\u001b[49m[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcondition\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124musefulCount\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      2\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment_clean_ss\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount_word\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount_unique_word\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount_letters\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount_punctuations\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount_words_upper\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount_words_title\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount_stopwords\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_word_len\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview_clean\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      5\u001b[0m test_data_features \u001b[38;5;241m=\u001b[39m df_test[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcondition\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124musefulCount\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment_clean_ss\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount_word\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount_unique_word\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount_letters\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount_punctuations\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount_words_upper\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount_words_title\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount_stopwords\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_word_len\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview_clean\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      9\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_data_features = pipeline.fit_transform(train_data_features)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_train' is not defined"
     ]
    }
   ],
   "source": [
    "train_data_features = df_train[['condition', 'usefulCount', 'sentiment', 'day', 'month', 'Year',\n",
    "                'sentiment_clean_ss', 'count_word', 'count_unique_word', 'count_letters',\n",
    "                'count_punctuations', 'count_words_upper', 'count_words_title',\n",
    "                'count_stopwords', 'mean_word_len', 'rating', 'review_clean']]\n",
    "test_data_features = df_test[['condition', 'usefulCount', 'sentiment', 'day', 'month', 'Year',\n",
    "                'sentiment_clean_ss', 'count_word', 'count_unique_word', 'count_letters',\n",
    "                'count_punctuations', 'count_words_upper', 'count_words_title',\n",
    "                'count_stopwords', 'mean_word_len', 'rating', 'review_clean']]\n",
    "%time train_data_features = pipeline.fit_transform(train_data_features)\n",
    "%time test_data_features = pipeline.fit_transform(test_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61afaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Bidirectional, LSTM, BatchNormalization, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3185899c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 14:47:32.169308: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-04-01 14:47:32.169345: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-04-01 14:47:32.169368: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (VM-20-12-ubuntu): /proc/driver/nvidia/version does not exist\n",
      "2022-04-01 14:47:32.169595: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import random\n",
    "\n",
    "# 1. Dataset\n",
    "y_train = df_train['rating']\n",
    "y_test = df_test['rating']\n",
    "solution = y_test.copy()\n",
    "\n",
    "# 2. Model Structure\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Dense(200, input_shape=(3426,)))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "\n",
    "model.add(keras.layers.Dense(300))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# 3. Model compile\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f876e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 200)               685400    \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 200)              800       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 200)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 200)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 300)               60300     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 300)              1200      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 300)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 300)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 777,901\n",
      "Trainable params: 776,901\n",
      "Non-trainable params: 1,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a02e116d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 4. Train model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_data_features \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_data_features\u001b[49m\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[1;32m      3\u001b[0m hist \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(train_data_features, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 5. Traing process\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data_features' is not defined"
     ]
    }
   ],
   "source": [
    "# 4. Train model\n",
    "train_data_features = train_data_features.toarray()\n",
    "hist = model.fit(train_data_features, y_train, epochs=10, batch_size=64)\n",
    "\n",
    "# 5. Traing process\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.set_ylim([0.0, 1.0])\n",
    "acc_ax.set_ylim([0.0, 1.0])\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 6. Evaluation\n",
    "loss_and_metrics = model.evaluate(test_data_features, y_test, batch_size=32)\n",
    "print('loss_and_metrics : ' + str(loss_and_metrics))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
